atari:
  env_wrapper:
    - stable_baselines3.common.atari_wrappers.AtariWrapper
  frame_stack: 4
  policy: 'CnnPolicy'
  n_envs: 16
  n_timesteps: !!float 1e7
  ent_coef: 0.01
  vf_coef: 0.25
  policy_kwargs: "dict(optimizer_class=RMSpropTFLike, optimizer_kwargs=dict(eps=1e-5))"

CartPole-v1:
  n_envs: 8
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  ent_coef: 0.0

LunarLander-v2:
  n_envs: 8
  n_timesteps: !!float 2e5
  policy: 'MlpPolicy'
  gamma: 0.995
  n_steps: 5
  learning_rate: lin_0.00083
  ent_coef: 0.00001

MountainCar-v0:
  normalize: true
  n_envs: 16
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  ent_coef: .0

Acrobot-v1:
  normalize: true
  n_envs: 16
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  ent_coef: .0

# Almost tuned
Pendulum-v0:
  normalize: True
  n_envs: 8
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_7e-4
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
LunarLanderContinuous-v2:
  normalize: true
  n_envs: 4
  n_timesteps: !!float 5e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_7e-4
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
MountainCarContinuous-v0:
  # env_wrapper: utils.wrappers.PlotActionWrapper
  normalize: true
  n_envs: 4
  n_steps: 100
  n_timesteps: !!float 1e5
  policy: 'MlpPolicy'
  ent_coef: 0.0
  use_sde: True
  sde_sample_freq: 16
  policy_kwargs: "dict(log_std_init=0.0, ortho_init=False)"

# Tuned
BipedalWalker-v3:
  normalize: true
  n_envs: 16
  n_timesteps: !!float 5e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.00096
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
BipedalWalkerHardcore-v3:
  normalize: true
  n_envs: 32
  n_timesteps: !!float 20e7
  policy: 'MlpPolicy'
  ent_coef: 0.001
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.0008
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
HalfCheetahBulletEnv-v0:
  env_wrapper: sb3_contrib.common.wrappers.TimeFeatureWrapper
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  # Both works
  learning_rate: lin_0.00096
  # learning_rate: !!float 3e-4
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False, full_std=True)"

Walker2DBulletEnv-v0:
  env_wrapper: sb3_contrib.common.wrappers.TimeFeatureWrapper
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.00096
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# normalize: true
# n_envs: 4
# n_timesteps: !!float 2e6
# policy: 'MlpPolicy'
# ent_coef: 0.0
# max_grad_norm: 0.5
# n_steps: 32
# gae_lambda: 0.9
# vf_coef: 0.4
# gamma: 0.99
# use_rms_prop: True
# normalize_advantage: False
# learning_rate: 0.0002
# use_sde: True
# policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
AntBulletEnv-v0:
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.00096
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned
HopperBulletEnv-v0:
  env_wrapper: sb3_contrib.common.wrappers.TimeFeatureWrapper
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.00096
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# Tuned but unstable
# Not working without SDE?
ReacherBulletEnv-v0:
  env_wrapper: sb3_contrib.common.wrappers.TimeFeatureWrapper
  normalize: true
  n_envs: 4
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  ent_coef: 0.0
  max_grad_norm: 0.5
  n_steps: 8
  gae_lambda: 0.9
  vf_coef: 0.4
  gamma: 0.99
  use_rms_prop: True
  normalize_advantage: False
  learning_rate: lin_0.0008
  use_sde: True
  policy_kwargs: "dict(log_std_init=-2, ortho_init=False)"

# ShipNav-v0:
#  n_envs: 8
#  n_timesteps: !!float 2e6
#  policy: 'MlpPolicy'
#  gamma: 0.999
#  normalize_advantage: False
#  max_grad_norm: 0.7
#  use_rms_prop: False
#  gae_lambda: 0.98
#  n_steps: 256
#  #lr_schedule: constant
#  learning_rate: 0.0003851665491403252
#  ent_coef: 9.597587134393752e-08
#  vf_coef: 0.19461625138465544
#  policy_kwargs: 'dict(activation_fn=nn.Tanh, ortho_init=True, net_arch=[dict(pi=[64, 64], vf=[64, 64])])'

# ShipNav-v0:
#   n_envs: 8
#   n_timesteps: !!float 2e7
#   policy: 'MlpPolicy'
#   gamma: 0.999
#   normalize_advantage: False
#   max_grad_norm: 0.6
#   use_rms_prop: True
#   gae_lambda: 0.98
#   n_steps: 64
#   #lr_schedule: constant
#   learning_rate: 0.0003851665491403252
#   ent_coef: 9.597587134393752e-08
#   vf_coef: 0.7
#   policy_kwargs: 'dict(activation_fn=nn.Tanh, ortho_init=True, net_arch=[dict(pi=[256, 256], vf=[256, 256])])'

# With only +10 and 10 rocks, 60 frames
# ShipNav-v0:
#   n_envs: 8
#   n_timesteps: !!float 1e6
#   policy: 'MlpPolicy'
#   gamma: 0.99
#   normalize_advantage: True
#   max_grad_norm: 0.3
#   use_rms_prop: True
#   gae_lambda: 1.0
#   n_steps: 32
#   #lr_schedule: constant
#   learning_rate: lin_0.0010057078670446065
#   ent_coef: 1.5356075354834543e-06
#   vf_coef: 0.788982467547876
#   policy_kwargs: 'dict(activation_fn=nn.ReLU, ortho_init=True, net_arch=[dict(pi=[256, 256], vf=[256, 256])])'

# With +10, -1 (if hit or no ep end) 
# ShipNav-v0:
#   n_envs: 8
#   n_timesteps: !!float 2e7
#   policy: 'MlpPolicy'
#   gamma: 0.999
#   normalize_advantage: True
#   max_grad_norm: 0.6
#   use_rms_prop: True
#   gae_lambda: 0.99
#   n_steps: 64
#   #lr_schedule: constant
#   learning_rate: 7.40140782178084e-05
#   ent_coef: 0.08340198722150012
#   vf_coef: 0.6715509627672132
#   policy_kwargs: 'dict(activation_fn=nn.ReLU, ortho_init=True, net_arch=[dict(pi=[256, 256], vf=[256, 256])])'

# 60 trial opt for +10 (30 rocks obs)
ShipNav-v0:
  n_envs: 8
  n_timesteps: !!float 1e8
  policy: 'MlpPolicy'
  gamma: 0.995
  normalize_advantage: True
  max_grad_norm: 2
  use_rms_prop: False
  gae_lambda: 0.99
  n_steps: 256
  #lr_schedule: constant
  learning_rate: 1.0454402147184612e-05
  ent_coef: 0.07123205384725213
  vf_coef: 0.4650581927897999
  policy_kwargs: 'dict(activation_fn=nn.ReLU, ortho_init=False, net_arch=[dict(pi=[256, 256, 64, 64], vf=[256, 256, 64, 64])])'


# 50 rocks hyper param search
ShipNav-v1:
  n_envs: 8
  n_timesteps: !!float 1e8
  policy: 'MlpPolicy'
  gamma: 0.9999
  normalize_advantage: True
  max_grad_norm: 0.8
  use_rms_prop: False
  gae_lambda: 0.9
  n_steps: 1024
  #lr_schedule: constant
  learning_rate: lin_0.06847428335486011
  ent_coef: 2.080342673475379e-08
  vf_coef: 0.06947756247380976
  policy_kwargs: 'dict(activation_fn=nn.ReLU, ortho_init=True, net_arch=[dict(pi=[256, 256], vf=[256, 256])])'
